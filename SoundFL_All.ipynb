{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h842r0c4X8N6"
      },
      "source": [
        "# **Introduction to FedDalf: Federated Domain Adaptation with Lifelong Learning**\n",
        "\n",
        "\n",
        "Welcome to this Colab tutorial on federated learning using the FedDalf method!\n",
        "\n",
        "In this notebook, we will build a federated learning system using FedDalf and PyTorch. In Part 1, we will set up the model training pipeline and data loading with PyTorch. In Part 2, we will introduce FedDalf, a cutting-edge approach that integrates federated learning with domain adaptation and lifelong learning to enhance model performance across different domains.\n",
        "\n",
        "Explore FedDalf on GitHub ‚≠êÔ∏è  to ask questions and get help.\n",
        "\n",
        "Let's get started! üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8JVBIBrX8N9"
      },
      "source": [
        "## Step 0: Preparation\n",
        "\n",
        "Before we begin with any actual code, let's make sure that we have everything we need."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubt90RskX8N9"
      },
      "source": [
        "## Installing dependencies\n",
        "\n",
        "Setup and Preprocessing for Federated Learning with `FedDalf` in `PyTorch` and `TensorFlow`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fAJwcdjEjOi4",
        "outputId": "038421e4-8b72-48ff-b785-e457dbef3589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: cryptography 44.0.3\n",
            "Uninstalling cryptography-44.0.3:\n",
            "  Successfully uninstalled cryptography-44.0.3\n",
            "Found existing installation: numpy 2.2.6\n",
            "Uninstalling numpy-2.2.6:\n",
            "  Successfully uninstalled numpy-2.2.6\n",
            "Collecting cryptography==44.0.3\n",
            "  Using cached cryptography-44.0.3-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography==44.0.3) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography==44.0.3) (2.23)\n",
            "Using cached cryptography-44.0.3-cp39-abi3-manylinux_2_34_x86_64.whl (4.2 MB)\n",
            "Installing collected packages: cryptography\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flwr 1.23.0 requires numpy<3.0.0,>=1.26.0, which is not installed.\n",
            "pyopenssl 24.2.1 requires cryptography<44,>=41.0.5, but you have cryptography 44.0.3 which is incompatible.\n",
            "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cryptography-44.0.3\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "2aeed229fc2a42648dee31ffe5df297c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imgaug==0.4.0 in /usr/local/lib/python3.12/dist-packages (0.4.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mdrive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PackagePath' object has no attribute '_drv'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3824541511.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Install imgaug without its dependencies to preserve numpy==1.26.4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install imgaug==0.4.0 --no-deps'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Reinstall numpy==1.26.4 with --force-reinstall to ensure it's the final version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m     opt_names = {\n\u001b[0;32m--> 958\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmodulename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    959\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m     }\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mparts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    704\u001b[0m         \"\"\"An object providing sequence-like access to the\n\u001b[1;32m    705\u001b[0m         components in the filesystem path.\"\"\"\n\u001b[0;32m--> 706\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrive\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrive\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mdrive\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m_load_parts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdrv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# --- Installation des d√©pendances et Imports / Dependency Installation and Imports ---\n",
        "import sys\n",
        "\n",
        "# Uninstall potentially conflicting versions\n",
        "!pip uninstall -y cryptography numpy\n",
        "\n",
        "# Install specific versions to avoid conflicts\n",
        "# Pin cryptography to a version compatible with flwr (e.g., 44.0.3 works with flwr 1.23.0)\n",
        "!pip install cryptography==44.0.3\n",
        "\n",
        "# Install numpy version compatible with imgaug 0.4.0\n",
        "!pip install numpy==1.26.4\n",
        "\n",
        "# Install other dependencies\n",
        "# Do not install imgaug with its dependencies yet to prevent numpy upgrade\n",
        "!pip install -q flwr[simulation] tensorflow matplotlib smote_variants tfds-nightly scipy\n",
        "\n",
        "# Install imgaug without its dependencies to preserve numpy==1.26.4\n",
        "!pip install imgaug==0.4.0 --no-deps\n",
        "\n",
        "# Reinstall numpy==1.26.4 with --force-reinstall to ensure it's the final version\n",
        "!pip install --force-reinstall numpy==1.26.4\n",
        "\n",
        "# Ensure flwr is up to date (this should not re-install numpy if 1.26.4 is already there)\n",
        "!pip install -U 'flwr[simulation]'\n",
        "\n",
        "print(\"Dependencies installed.\")\n",
        "\n",
        "# --- V√©rification et Imports / Check and Imports ---\n",
        "try:\n",
        "    # Attempt import to check if restart is needed\n",
        "    import flwr as fl\n",
        "    import imgaug.augmenters as iaa\n",
        "    from cryptography.hazmat.bindings._rust import PKCS7UnpaddingContext\n",
        "except ImportError:\n",
        "    print(\"\\n\" + \"!\"*80)\n",
        "    print(\"\\u26a0\\ufe0f CRITICAL: RUNTIME RESTART REQUIRED / RED√âMARRAGE REQUIS \\u26a0\\ufe0f\")\n",
        "    print(\"The 'cryptography' or 'numpy' library was updated, but old versions are loaded in memory.\")\n",
        "    print(\"1. Go to: Runtime > Restart session (or Restart Runtime).\")\n",
        "    print(\"2. Run this cell again.\")\n",
        "    print(\"\\nLa biblioth√®que 'cryptography' ou 'numpy' a √©t√© mise √† jour, mais l'ancienne version est charg√©e.\")\n",
        "    print(\"1. Allez dans : Ex√©cution > Red√©marrer la session.\")\n",
        "    print(\"2. Relancez cette cellule.\")\n",
        "    print(\"!\"*80 + \"\\n\")\n",
        "    sys.exit(\"Please restart the runtime / Veuillez red√©marrer la session.\")\n",
        "\n",
        "NUM_CLASSES = 11\n",
        "CLASS_LIST=['car_horn','dog_bark','gun_shot','siren','frog','thunder','cat','rooster','water','cock','baby']\n",
        "IMG_SIZE = 28\n",
        "input_dim = (16, 8, 1)\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import PIL\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.io\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import re\n",
        "import math\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.applications import resnet50, VGG16, InceptionV3, Xception\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from flwr.common import Metrics\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer, OneHotEncoder, RobustScaler, LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import silhouette_score, roc_curve, auc, roc_auc_score, confusion_matrix\n",
        "from sklearn.datasets import load_wine\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import OrderedDict\n",
        "from typing import List, Tuple\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.random_projection import SparseRandomProjection\n",
        "from skimage import data\n",
        "from skimage.transform import rotate\n",
        "from sklearn.utils import shuffle\n",
        "from PIL import Image\n",
        "from keras.layers import Dense\n",
        "\n",
        "print(\"Libraries imported successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive for Data Access in Colab."
      ],
      "metadata": {
        "id": "F2LXfw-BPaUu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-9Nu6ruX8N-",
        "outputId": "a47e582c-34cc-4ea0-d568-0c420e65f3e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# --- Montage du Drive / Drive Mounting ---\n",
        "from google.colab import drive\n",
        "\n",
        "# Monte le Google Drive dans le r√©pertoire /content/drive\n",
        "# Mounts Google Drive to the /content/drive directory\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Management"
      ],
      "metadata": {
        "id": "y0vDj8fMWrkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loading Functions"
      ],
      "metadata": {
        "id": "Hqz7YlrcWuL-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vz3R7bXs7tqv"
      },
      "outputs": [],
      "source": [
        "def charger_donnees(dossier):\n",
        "    \"\"\"\n",
        "    Charge les donn√©es (features et labels) √† partir d'un dossier sp√©cifi√©.\n",
        "    Les fichiers doivent √™tre au format .npy.\n",
        "\n",
        "    Loads data (features and labels) from a specified folder.\n",
        "    Files must be in .npy format.\n",
        "    \"\"\"\n",
        "    # Parcourir le dossier / Browse the folder\n",
        "    for fichier in os.listdir(dossier):\n",
        "        if fichier.endswith('.npy'):  # S'assurer que le fichier est un fichier numpy / Ensure the file is a numpy file\n",
        "            chemin_fichier = os.path.join(dossier, fichier)\n",
        "            # Charger les caract√©ristiques et les √©tiquettes √† partir du fichier / Load features and labels from the file\n",
        "            if 'features' in fichier:\n",
        "                features = np.load(chemin_fichier)\n",
        "            elif 'labels' in fichier:\n",
        "                labels = np.load(chemin_fichier)\n",
        "    return features, labels\n",
        "\n",
        "# Fonction pour cr√©er des donn√©es clients / Function to create client data\n",
        "def make_client_data():\n",
        "    \"\"\"\n",
        "    Charge et pr√©pare les donn√©es pour chaque client simul√©.\n",
        "    Effectue le split train/test et le reshape des images.\n",
        "\n",
        "    Loads and prepares data for each simulated client.\n",
        "    Performs train/test split and image reshaping.\n",
        "    \"\"\"\n",
        "    client_folders = [\n",
        "                      \"/content/drive/MyDrive/numpyDataset\",\n",
        "                      \"/content/drive/MyDrive/urbansound8k\",\n",
        "                      # \"/content/drive/MyDrive/SoundFedLearning/urbansound8k\",\n",
        "                      # \"/content/drive/MyDrive/SoundFedLearning/urbansound8k\",\n",
        "                      ]\n",
        "    client_data = []\n",
        "    for folder in client_folders:\n",
        "        X, Y = charger_donnees(folder)\n",
        "        Y = to_categorical(Y)\n",
        "        # Split des donn√©es en train et test / Split data into train and test\n",
        "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 1)\n",
        "        # Redimensionnement pour correspondre √† l'entr√©e du mod√®le (16x8) / Resizing to match model input (16x8)\n",
        "        X_train = X_train.reshape(len(X_train), 16, 8, 1)\n",
        "        X_test = X_test.reshape(len(X_test), 16, 8, 1)\n",
        "        client_data.append(([X_train], [Y_train], [X_test], [Y_test]))\n",
        "    return client_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Manipulation and Processing Functions:"
      ],
      "metadata": {
        "id": "1VlcqcqTWeIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resize(x_train,IMG_SIZE):\n",
        "        \"\"\"\n",
        "        Redimensionne une liste d'images vers une taille donn√©e.\n",
        "        Resizes a list of images to a given size.\n",
        "        \"\"\"\n",
        "        x_train_resized=[]\n",
        "        for i in range(len(x_train)):\n",
        "            x_train_resized.append(cv2.resize(x_train[i],(IMG_SIZE, IMG_SIZE)))\n",
        "        return x_train_resized"
      ],
      "metadata": {
        "id": "yPVMhoXgXL_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Management Functions:"
      ],
      "metadata": {
        "id": "22hPHixNWiYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def number_of_labels(y_train):\n",
        "  \"\"\"\n",
        "  Compte le nombre d'√©chantillons √©tiquet√©s (non-NaN).\n",
        "  Counts the number of labeled samples (non-NaN).\n",
        "  \"\"\"\n",
        "  size=len(y_train)\n",
        "  nb=0\n",
        "  if isinstance(y_train[0],np.ndarray) or isinstance(y_train[0],list) :\n",
        "    for i in range(size):\n",
        "      if np.isnan(y_train[i][0]):\n",
        "        continue\n",
        "      nb=nb+1\n",
        "  else:\n",
        "    for i in range(size):\n",
        "      if np.isnan(y_train[i]):\n",
        "        continue\n",
        "      nb=nb+1\n",
        "\n",
        "  return nb"
      ],
      "metadata": {
        "id": "h5icngcDXvkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. History Saving and Loading"
      ],
      "metadata": {
        "id": "oFV2C0qzW2WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour sauvegarder l'historique fit et val round par round {loss, val-loss, acc, val-acc}\n",
        "# Function to save round by round on-device fit and val history {loss, val-loss, acc, val-acc}\n",
        "def saving_history_dict(history_dict,path):\n",
        "  try:\n",
        "    file_dict = open(path, 'a')\n",
        "    file_dict.write(str(history_dict))\n",
        "    file_dict.write('\\n')\n",
        "    file_dict.close()\n",
        "    print(\"History data saved\")\n",
        "  except:\n",
        "    print(\"Unable to write to file\")\n",
        "\n",
        "def load_list_from_file(path,round): #for fit and val only\n",
        "  \"\"\"\n",
        "  Charge une liste √† partir d'un fichier pour un round sp√©cifique.\n",
        "  Loads a list from a file for a specific round.\n",
        "  \"\"\"\n",
        "  with open(path) as f:\n",
        "    history={}\n",
        "    for line_data in f:\n",
        "        line_dict={}\n",
        "        line_data=re.sub('[\\']', '\"',line_data) #replace ' by \" in the string\n",
        "        line_dict=json.loads(line_data)\n",
        "        entree=list(line_dict.values())\n",
        "        if entree[0]==round:\n",
        "           return entree[1]\n",
        "    return []\n",
        "\n",
        "def update_list(filename,round,actual_list):\n",
        "  \"\"\"\n",
        "  Met √† jour la liste des statuts, labels et total_size. Le round doit √™tre le pr√©c√©dent.\n",
        "  Update the list of status, labels and total_size. round must be the previous round.\n",
        "  \"\"\"\n",
        "  if round==0:\n",
        "    return actual_list\n",
        "\n",
        "  last_list=load_list_from_file(filename,round)\n",
        "  #print(\"Ancienne liste===>\",last_list)\n",
        "  #print(\"List_actuelle===>\",actual_list)\n",
        "  for i in range(len(last_list)):\n",
        "    if last_list[i]!=actual_list[i] and actual_list[i]!=-1:\n",
        "      last_list[i]=actual_list[i]\n",
        "  print(\"Retourne==>\",last_list)\n",
        "  return last_list"
      ],
      "metadata": {
        "id": "lbQbxyZAYBpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Utility Functions (for Clients and Categories)"
      ],
      "metadata": {
        "id": "v8bi6jYYXz3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Client Management Functions:"
      ],
      "metadata": {
        "id": "itymwEsVYih8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_normalized_list(nbTotalClient,clients_name,client_x):\n",
        "  \"\"\"\n",
        "  Retourne une liste normalis√©e de taille nbTotalClient.\n",
        "  Returns a normalized list of size nbTotalClient.\n",
        "  \"\"\"\n",
        "  normalized_x=[-1 for _ in range(nbTotalClient)]\n",
        "  for client,elt in zip(clients_name,client_x):\n",
        "    normalized_x[client]=elt\n",
        "  return normalized_x\n",
        "\n",
        "def get_selected_categorie_set(nbTotalClient,clients_name,clients_status,categorie_list):\n",
        "  \"\"\"\n",
        "  S√©lectionne les clients en fonction de leur statut et des cat√©gories requises.\n",
        "  0: totalement √©tiquet√©, 1: partiellement √©tiquet√©, 2: autres.\n",
        "\n",
        "  Selects clients based on their status and required categories.\n",
        "  0: fully labeled, 1: partially labeled, 2: others.\n",
        "  \"\"\"\n",
        "  normalized_status=get_normalized_list(nbTotalClient,clients_name,clients_status) # Normalize the status list\n",
        "  selected_clients=[0 for _ in range(nbTotalClient)]\n",
        "  for i in range(len(normalized_status)):\n",
        "    if normalized_status[i] in categorie_list:\n",
        "      selected_clients[i]=1\n",
        "  return selected_clients"
      ],
      "metadata": {
        "id": "fE2yWSaqYYbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dictionary Creation Functions:"
      ],
      "metadata": {
        "id": "3BdYAe1rYkNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cr√©e un dictionnaire √† partir d'une liste de cl√©s et d'une liste de valeurs\n",
        "# Creates a dictionary from a list of keys and a list of values\n",
        "def create_dictionary(Names_list,Values_list):\n",
        "  dictionary={}\n",
        "  for key,value in zip(Names_list,Values_list):\n",
        "    dictionary[key]=value\n",
        "  return dictionary"
      ],
      "metadata": {
        "id": "5EKCOZ5AYm0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Audio CNN Model for Classification"
      ],
      "metadata": {
        "id": "4BHcMxuVaioS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soEzF8rhBhkR"
      },
      "outputs": [],
      "source": [
        "# Fonction pour cr√©er un mod√®le Keras\n",
        "# Function to create a Keras model\n",
        "def create_keras_model():\n",
        "    \"\"\"\n",
        "    D√©finit l'architecture du mod√®le CNN.\n",
        "    Defines the CNN model architecture.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    # First convolutional layer with 64 filters, 3x3 kernel, 'same' padding, and ReLU activation\n",
        "    model.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", input_shape=input_dim))\n",
        "    # Max pooling layer to reduce the spatial dimensions by half\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "    # Second convolutional layer with 128 filters, 3x3 kernel, 'same' padding, and ReLU activation\n",
        "    model.add(Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "    # Another max pooling layer\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "    # Dropout layer with a dropout rate of 0.1 to prevent overfitting\n",
        "    model.add(Dropout(0.1))\n",
        "    # Flatten the 3D output from convolutional layers to a 1D vector before the dense layers\n",
        "    model.add(Flatten())\n",
        "    # Fully connected dense layer with 1024 units and ReLU activation\n",
        "    model.add(Dense(1024, activation=\"relu\"))\n",
        "    # Output layer with 9 units and softmax activation for multi-class classification\n",
        "    model.add(Dense(9, activation=\"softmax\"))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nME90EgbsDJ"
      },
      "source": [
        "Aggregating Client Data for Training and Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMcIPFvV0RqI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b6b1d61-aa78-4dd8-8da5-6baf7ca77e06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chargement des donn√©es... / Loading data...\n"
          ]
        }
      ],
      "source": [
        "# Agr√©gation des donn√©es clients pour l'entra√Ænement et le test\n",
        "# Aggregating Client Data for Training and Testing\n",
        "\n",
        "# --- D√©finitions de secours / Fallback definitions ---\n",
        "if 'charger_donnees' not in globals():\n",
        "    def charger_donnees(dossier):\n",
        "        features, labels = [], []\n",
        "        if not os.path.exists(dossier): return np.array([]), np.array([])\n",
        "        for fichier in os.listdir(dossier):\n",
        "            if fichier.endswith('.npy'):\n",
        "                path = os.path.join(dossier, fichier)\n",
        "                if 'features' in fichier: features = np.load(path)\n",
        "                elif 'labels' in fichier: labels = np.load(path)\n",
        "        return features, labels\n",
        "\n",
        "if 'make_client_data' not in globals():\n",
        "    def make_client_data():\n",
        "        client_folders = [\"/content/drive/MyDrive/numpyDataset\", \"/content/drive/MyDrive/urbansound8k\"]\n",
        "        client_data = []\n",
        "        for folder in client_folders:\n",
        "            if os.path.exists(folder):\n",
        "                X, Y = charger_donnees(folder)\n",
        "                if len(X) > 0:\n",
        "                    Y = to_categorical(Y)\n",
        "                    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=1)\n",
        "                    X_train = X_train.reshape(len(X_train), 16, 8, 1)\n",
        "                    X_test = X_test.reshape(len(X_test), 16, 8, 1)\n",
        "                    client_data.append(([X_train], [Y_train], [X_test], [Y_test]))\n",
        "        return client_data\n",
        "\n",
        "# V√©rification et chargement / Check and load\n",
        "if 'data_all' not in globals():\n",
        "    print(\"Chargement des donn√©es... / Loading data...\")\n",
        "    data_all = make_client_data()\n",
        "\n",
        "all_X_train, all_y_train, all_X_test, all_y_test = [], [], [], []\n",
        "if 'data_all' in globals() and data_all:\n",
        "    for data in data_all:\n",
        "        x_train, y_train, x_test, y_test = data\n",
        "        all_X_train.append(np.array(x_train[0]))\n",
        "        all_y_train.append(np.array(y_train[0]))\n",
        "        all_X_test.append(np.array(x_test[0]))\n",
        "        all_y_test.append(np.array(y_test[0]))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Aucune donn√©e charg√©e. V√©rifiez les chemins d'acc√®s sur Google Drive. / No data loaded. Check Google Drive paths.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering and Aggregating Clients Based on Selected Classes"
      ],
      "metadata": {
        "id": "hQpn-BJtbSVb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwzis1o2MogO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dec96a8-2117-43fb-82fe-89f4119345a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 ==>in (Client s√©lectionn√© / Client selected)\n",
            "1 ==>in (Client s√©lectionn√© / Client selected)\n",
            "Clients retenus / Retained clients: [0, 1]\n",
            "Distribution Client 0: [0, 673, 0, 687, 0, 0, 677, 0, 691, 0, 0, 0, 0, 0, 0, 0, 0], Total: 2728\n",
            "Distribution Client 1: [0, 350, 0, 752, 0, 0, 274, 0, 696, 0, 0, 0, 0, 0, 0, 0, 0], Total: 2072\n",
            "Client 0 Shape: X=(2728, 16, 8, 1), Y=(2728, 9)\n",
            "Client 1 Shape: X=(2072, 16, 8, 1), Y=(2072, 10)\n"
          ]
        }
      ],
      "source": [
        "# --- Pr√©requis : Chargement des donn√©es / Prerequisites: Data Loading ---\n",
        "if 'all_y_train' not in globals():\n",
        "    print(\"Chargement des donn√©es manquant, ex√©cution de make_client_data()... / Missing data loading, running make_client_data()...\")\n",
        "    # On suppose que make_client_data est d√©fini ou on le d√©finit ici si n√©cessaire\n",
        "    if 'make_client_data' not in globals():\n",
        "         # Fallback simple si la cellule pr√©c√©dente n'est pas ex√©cut√©e\n",
        "         print(\"Fonction make_client_data non trouv√©e. Veuillez ex√©cuter les cellules de d√©finition des donn√©es.\")\n",
        "    else:\n",
        "         data_all = make_client_data()\n",
        "         all_X_train, all_y_train, all_X_test, all_y_test = [], [], [], []\n",
        "         for data in data_all:\n",
        "            x_train, y_train, x_test, y_test = data\n",
        "            all_X_train.append(np.array(x_train[0]))\n",
        "            all_y_train.append(np.array(y_train[0]))\n",
        "            all_X_test.append(np.array(x_test[0]))\n",
        "            all_y_test.append(np.array(y_test[0]))\n",
        "\n",
        "def renew_list():\n",
        "  \"\"\"\n",
        "  R√©initialise une liste de compteurs pour les 17 classes potentielles.\n",
        "  Resets a list of counters for the 17 potential classes.\n",
        "  \"\"\"\n",
        "  L=[]\n",
        "  for i in range(17):\n",
        "    L.append(0)\n",
        "  return L\n",
        "\n",
        "# Indices des classes sp√©cifiques que nous voulons conserver/analyser\n",
        "# Indices of specific classes we want to keep/analyze\n",
        "indexed_slices=[1,3,6,8]\n",
        "clients_to_consider=[]\n",
        "\n",
        "# 1. Identification des clients valides / Identification of valid clients\n",
        "# On parcourt tous les clients pour voir s'ils poss√®dent des donn√©es pour les classes cibles (indexed_slices)\n",
        "if 'all_y_train' in globals():\n",
        "    for client,elt in enumerate(all_y_train):\n",
        "      class_flag=True\n",
        "      L=renew_list()\n",
        "      for e in elt:\n",
        "        indix=np.argmax(e)\n",
        "        L[indix]=L[indix]+1\n",
        "\n",
        "      # V√©rifie si le client a au moins un exemple pour chaque classe cible\n",
        "      for i in indexed_slices:\n",
        "        if L[i]==0:\n",
        "          class_flag=False\n",
        "          break\n",
        "      if class_flag==False:\n",
        "        print(client,'==>out (Client rejet√© / Client rejected)')\n",
        "      else:\n",
        "        clients_to_consider.append(client)\n",
        "        print(client,'==>in (Client s√©lectionn√© / Client selected)')\n",
        "\n",
        "    print(\"Clients retenus / Retained clients:\", clients_to_consider)\n",
        "\n",
        "    # 2. Filtrage des donn√©es / Data Filtering\n",
        "    new_all_X_train, new_all_y_train, new_all_X_test, new_all_y_test = [], [], [], []\n",
        "    client=0\n",
        "    for x_train,y_train in zip(all_X_train,all_y_train):\n",
        "      x_t,y_t=[],[]\n",
        "      if client in clients_to_consider:\n",
        "        for x_,y_ in zip(x_train,y_train):\n",
        "          if np.argmax(y_) in indexed_slices:\n",
        "            x_t.append(x_)\n",
        "            y_t.append(y_)\n",
        "      client=client+1\n",
        "      if len(x_t)!=0:\n",
        "        new_all_X_train.append(np.array(x_t))\n",
        "        new_all_y_train.append(np.array(y_t))\n",
        "\n",
        "    # 3. V√©rification de la distribution / Distribution check\n",
        "    for client,elt in enumerate(new_all_y_train):\n",
        "      L=renew_list()\n",
        "      for e in elt:\n",
        "        indix=np.argmax(e)\n",
        "        L[indix]=L[indix]+1\n",
        "      print(f\"Distribution Client {client}: {L}, Total: {sum(L)}\")\n",
        "\n",
        "    for i in range(len(new_all_X_train)):\n",
        "      print(f\"Client {i} Shape: X={new_all_X_train[i].shape}, Y={new_all_y_train[i].shape}\")\n",
        "else:\n",
        "    print(\"Erreur: Donn√©es non charg√©es (all_y_train).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcMTcM-SbxGR"
      },
      "source": [
        "MODEL GLOBAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdwpgAwUyNn3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "81ed80b0-08ed-4363-8880-a864b4eb85ef"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "`np.sctypes` was removed in the NumPy 2.0 release. Access dtypes explicitly instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-21711858.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimgaug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maugmenters\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0miaa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisturb_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/imgaug/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# classes/functions, hence always place the other imports below this so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# the deprecated stuff gets overwritten as much as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimgaug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgaug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimgaug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmentables\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0maugmentables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/imgaug/imgaug.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# `dtype.type in  NP_FLOAT_TYPES` do not just use `dtype in NP_FLOAT_TYPES` as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# that would fail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mNP_FLOAT_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msctypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"float\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mNP_INT_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msctypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"int\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mNP_UINT_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msctypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"uint\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m__expired_attributes__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m    398\u001b[0m                 \u001b[0;34mf\"`np.{attr}` was removed in the NumPy 2.0 release. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0;34mf\"{__expired_attributes__[attr]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: `np.sctypes` was removed in the NumPy 2.0 release. Access dtypes explicitly instead."
          ]
        }
      ],
      "source": [
        "from keras.utils import split_dataset\n",
        "from imgaug import augmenters as iaa\n",
        "\n",
        "def disturb_labels(y_train,n):\n",
        "  \"\"\"\n",
        "  Simule des √©tiquettes manquantes en rempla√ßant les n derniers labels par NaN.\n",
        "  Simulates missing labels by replacing the last n labels with NaN.\n",
        "  \"\"\"\n",
        "  size=len(y_train)\n",
        "  if not(isinstance(y_train[0],np.ndarray)) and not(isinstance(y_train[0],list)) :\n",
        "    y_train=y_train.astype('float')\n",
        "  y_train_copy=y_train.copy()\n",
        "  indice_no_labels=size-n\n",
        "\n",
        "  if  n==0:\n",
        "    return y_train\n",
        "  if indice_no_labels<0 :\n",
        "    indice_no_labels=0\n",
        "  for i in range(indice_no_labels,size,1):\n",
        "    y_train_copy[i]=np.nan\n",
        "  return y_train_copy\n",
        "\n",
        "def generate_one_hotpot_vector(position,size):\n",
        "  \"\"\"\n",
        "  G√©n√®re un vecteur one-hot.\n",
        "  Generates a one-hot vector.\n",
        "  \"\"\"\n",
        "  vector=[0.0 for i in range(size)]\n",
        "  vector[position]=1.0\n",
        "  return vector\n",
        "\n",
        "def max_and_position(L):\n",
        "  \"\"\"\n",
        "  Retourne le max de la liste et sa position.\n",
        "  Returns the max of the list and its position.\n",
        "  \"\"\"\n",
        "  max_e=max(L)\n",
        "  return max_e,L.tolist().index(max_e)\n",
        "\n",
        "def map_predict(Y_pred,threshold):\n",
        "  \"\"\"\n",
        "  Transforme les probabilit√©s en vecteurs one-hot si la confiance d√©passe le seuil.\n",
        "  Transforms probabilities into one-hot vectors if confidence exceeds threshold.\n",
        "  \"\"\"\n",
        "  count=0\n",
        "  for i in range(len(Y_pred)):\n",
        "    size=len(Y_pred[i])\n",
        "    acc,position=max_and_position(Y_pred[i])\n",
        "    if acc>=threshold:\n",
        "      Y_pred[i]=generate_one_hotpot_vector(position,size)\n",
        "      count=count+1\n",
        "    else:\n",
        "      Y_pred[i]=np.nan\n",
        "  return Y_pred,count\n",
        "\n",
        "def update_y_train(Y_train,Y_pred):\n",
        "  \"\"\"\n",
        "  Corrige Y_pred avec les nouvelles pseudo-√©tiquettes provenant de Y_train.\n",
        "  Corrects Y_pred with new pseudo-labels from Y_train.\n",
        "  \"\"\"\n",
        "  assert len(Y_train)==len(Y_pred),\"Oh no! Y_train doesn't have the same size as Y_pred!\"\n",
        "  if isinstance(Y_train[0],np.ndarray) or isinstance(Y_train[0],list):\n",
        "    for i in range(len(Y_train)):\n",
        "      if np.isnan(Y_train[i][0])==False:\n",
        "        Y_pred[i]=Y_train[i]\n",
        "  else:\n",
        "    for i in range(len(Y_train)):\n",
        "      if np.isnan(Y_train[i])==False:\n",
        "        Y_pred[i]=Y_train[i]\n",
        "  return Y_pred\n",
        "\n",
        "\n",
        "def get_labeled_set(x_train,y_train):\n",
        "  \"\"\"\n",
        "  Retourne uniquement les √©chantillons √©tiquet√©s.\n",
        "  Returns only labeled samples.\n",
        "  \"\"\"\n",
        "  size=len(y_train)\n",
        "  y=[]\n",
        "  x=[]\n",
        "  if isinstance(y_train[0],np.ndarray) or isinstance(y_train[0],list):\n",
        "    for i in range(size):\n",
        "      if np.isnan(y_train[i][0])==False:\n",
        "        y.append(y_train[i])\n",
        "        x.append(x_train[i])\n",
        "  else:\n",
        "    for i in range(size):\n",
        "      if np.isnan(y_train[i])==False:\n",
        "        y.append(y_train[i])\n",
        "        x.append(x_train[i])\n",
        "  return np.array(x),np.array(y)\n",
        "\n",
        "def get_unlabeled_set(x_train,y_train):\n",
        "  \"\"\"\n",
        "  Retourne uniquement les √©chantillons non √©tiquet√©s.\n",
        "  Returns only unlabeled samples.\n",
        "  \"\"\"\n",
        "  size=len(y_train)\n",
        "  y=[]\n",
        "  x=[]\n",
        "  if isinstance(y_train[0],np.ndarray) or isinstance(y_train[0],list):\n",
        "    for i in range(size):\n",
        "      if np.isnan(y_train[i][0]):\n",
        "        y.append(y_train[i])\n",
        "        x.append(x_train[i])\n",
        "  else:\n",
        "    for i in range(size):\n",
        "      if np.isnan(y_train[i]):\n",
        "        y.append(y_train[i])\n",
        "        x.append(x_train[i])\n",
        "  return np.array(x),np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DikbBHClxLQt"
      },
      "outputs": [],
      "source": [
        "# --- Test de s√©paration des donn√©es (Client 0) / Data Split Test (Client 0) ---\n",
        "# V√©rification du split train/test sur le premier client.\n",
        "# Checking train/test split on the first client.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "if 'new_all_X_train' in globals() and len(new_all_X_train) > 0:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(new_all_X_train[0], new_all_y_train[0], test_size=0.33, random_state=42)\n",
        "    print(\"Split successful. X_train shape:\", X_train.shape)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Les donn√©es 'new_all_X_train' ne sont pas d√©finies. Veuillez ex√©cuter la cellule de filtrage des donn√©es pr√©c√©dente.\")\n",
        "    print(\"‚ö†Ô∏è 'new_all_X_train' data is not defined. Please run the previous data filtering cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvQkUF_ah1Mi"
      },
      "outputs": [],
      "source": [
        "# --- Configuration des chemins et param√®tres / Path and Parameter Setup ---\n",
        "import os\n",
        "\n",
        "#Change le chemin\n",
        "initial_path_all_users='/content/drive/MyDrive/FEDADL/history/'\n",
        "if not os.path.exists(initial_path_all_users):\n",
        "    os.makedirs(initial_path_all_users)\n",
        "\n",
        "# Check if new_all_X_train exists before using len()\n",
        "if 'new_all_X_train' in globals():\n",
        "    NUM_CLIENTS = len(new_all_X_train)\n",
        "else:\n",
        "    NUM_CLIENTS = 10 # Default fallback\n",
        "    print(\"NUM_CLIENTS set to default (10) because data is not loaded.\")\n",
        "\n",
        "# Define input_dim if not present (defaulting to the value used elsewhere)\n",
        "if 'input_dim' not in globals():\n",
        "    input_dim = (16, 8, 1)\n",
        "\n",
        "IMG_SHAPE=input_dim\n",
        "base_learning_rate = 0.0001\n",
        "def ecrire_dans_fichier(data,fichier_resultat):\n",
        "    with open(fichier_resultat, 'a') as fichier:\n",
        "        fichier.write(data + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNUSz4SWutlC"
      },
      "outputs": [],
      "source": [
        "# --- Configuration / Configuration ---\n",
        "if 'initial_path_all_users' not in globals():\n",
        "    initial_path_all_users = '/content/drive/MyDrive/FEDADL/history/'\n",
        "    if not os.path.exists(initial_path_all_users):\n",
        "        os.makedirs(initial_path_all_users)\n",
        "\n",
        "if 'base_learning_rate' not in globals():\n",
        "    base_learning_rate = 0.0001\n",
        "\n",
        "NUM_CLIENTS = 10\n",
        "FRACTION_CLIENTS=1.0 # Fraction des clients s√©lectionn√©s √† chaque round (1.0 = tous)\n",
        "MINIMUM_CLIENTS=NUM_CLIENTS # Nombre minimum de clients requis\n",
        "EPOCHS=3\n",
        "NUM_CLASS=17\n",
        "NUM_ROUNDS=50\n",
        "initial_path= initial_path_all_users+'evaluation/' # Chemin de sauvegarde / Save path\n",
        "input_dim = (16, 8, 1)\n",
        "\n",
        "# --- D√©finition du mod√®le / Model Definition ---\n",
        "def create_keras_model():\n",
        "    \"\"\"\n",
        "    Cr√©e et compile le mod√®le CNN Keras.\n",
        "    Creates and compiles the Keras CNN model.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, (3, 3), padding = \"same\", activation = \"tanh\", input_shape = input_dim))\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "    model.add(Conv2D(128, (3, 3), padding = \"same\", activation = \"tanh\"))\n",
        "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1024, activation = \"tanh\"))\n",
        "    model.add(Dense(17, activation = \"softmax\"))\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model=create_keras_model()\n",
        "\n",
        "# --- D√©finition du Client Flower / Flower Client Definition ---\n",
        "class FlowerClient(fl.client.NumPyClient):\n",
        "  def __init__(self,model,train_data_X,train_data_Y,val_X,val_Y,cid):\n",
        "     self.model=model\n",
        "     self.train_data_X=train_data_X\n",
        "     self.train_data_Y=train_data_Y\n",
        "     self.val_X=val_X\n",
        "     self.val_Y=val_Y\n",
        "     self.cid=cid\n",
        "\n",
        "  def get_parameters(self, config):\n",
        "    return model.get_weights()\n",
        "\n",
        "  def fit(self, parameters, config):\n",
        "    client_name='client_'+str(self.cid)\n",
        "    self.model.set_weights(parameters)\n",
        "    history = self.model.fit(self.train_data_X, self.train_data_Y, epochs=EPOCHS, validation_data=(self.val_X, self.val_Y),verbose=0)\n",
        "\n",
        "    current_round=config['current_round']\n",
        "    dict={'round'+str(current_round):history.history}\n",
        "    path=initial_path+client_name+'.txt'\n",
        "    saving_history_dict(dict, path)\n",
        "\n",
        "    path_local_eval=initial_path+'Local_'+client_name+'.txt'\n",
        "    loss, acc = self.model.evaluate(self.val_X, self.val_Y,verbose=0)\n",
        "    hist={'Local_loss':[loss], 'Local_accuracy':[acc]}\n",
        "    saving_history_dict({'round'+str(current_round):hist}, path_local_eval)\n",
        "\n",
        "    return self.model.get_weights(), len(self.train_data_X), {'cid':self.cid}\n",
        "\n",
        "  def evaluate(self, parameters, config):\n",
        "    client_name='client_'+str(self.cid)\n",
        "    current_round=config['current_round']\n",
        "    self.model.set_weights(parameters)\n",
        "    Global_loss, Global_accuracy = self.model.evaluate(self.val_X, self.val_Y,verbose=0)\n",
        "\n",
        "    hist={'Global_loss':[Global_loss], 'Global_accuracy':[Global_accuracy]}\n",
        "    path=initial_path+'Eval_'+client_name+'.txt'\n",
        "    saving_history_dict({'round'+str(current_round):hist}, path)\n",
        "\n",
        "    try:\n",
        "      if current_round==NUM_ROUNDS:\n",
        "        model.save(initial_path+'model')\n",
        "    except: pass\n",
        "    return Global_loss, len(self.val_X), {\"cid\":self.cid,\"accuracy\": Global_accuracy,\"loss\":Global_loss,\"round\":config['current_round']}\n",
        "\n",
        "def client_fn(cid: str) -> FlowerClient:\n",
        "    if 'new_all_X_train' not in globals():\n",
        "        print(\"Erreur: Donn√©es non filtr√©es (new_all_X_train manque).\")\n",
        "        # Retourner un client factice ou g√©rer l'erreur\n",
        "        return None\n",
        "    x_train, x_test, y_train, y_test = train_test_split(new_all_X_train[int(cid)], new_all_y_train[int(cid)], test_size=0.2, random_state=42)\n",
        "    return FlowerClient(model, x_train,y_train,x_test,y_test,cid).to_client()\n",
        "\n",
        "# --- C√¥t√© Serveur / Server Side ---\n",
        "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
        "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
        "    losses=[num_examples * m[\"loss\"] for num_examples, m in metrics]\n",
        "    examples = [num_examples for num_examples, _ in metrics]\n",
        "    Global_accuracy=sum(accuracies) / sum(examples)\n",
        "    Global_loss=sum(losses) / sum(examples)\n",
        "\n",
        "    current_round=metrics[0][1][\"round\"] if metrics else 0\n",
        "    print(\"Total clients for evaluation:\",len(accuracies))\n",
        "\n",
        "    hist={'eval_loss':[Global_loss], 'eval_accuracy':[Global_accuracy]}\n",
        "    saving_history_dict({'round'+str(current_round):hist}, initial_path+'Evaluation.txt')\n",
        "    return {\"accuracy\": Global_accuracy}\n",
        "\n",
        "def fit_config(server_round: int):\n",
        "    return {\"batch_size\": 32, \"current_round\": server_round, \"local_epochs\": 3}\n",
        "\n",
        "def eval_config(server_round: int):\n",
        "    return {\"current_round\": server_round}\n",
        "\n",
        "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
        "    def configure_fit(self, server_round, parameters, client_manager):\n",
        "        print(\"Clients disponibles / Available clients:\",client_manager.all().keys())\n",
        "        client_fit_ins_list=super().configure_fit(server_round, parameters, client_manager)\n",
        "\n",
        "        selected_client=[client.cid for (client,_) in client_fit_ins_list]\n",
        "        clients_status=[1 if str(i) in selected_client else 0 for i in range(len(client_manager.all().keys()))]\n",
        "\n",
        "        saving_history_dict(create_dictionary(['round','status'],[server_round, clients_status]), initial_path+'selected.txt')\n",
        "        return client_fit_ins_list\n",
        "\n",
        "    def aggregate_fit(self,server_round,results,failures):\n",
        "        for _,parameters in results: print('Client:',parameters.metrics['cid'])\n",
        "        return super().aggregate_fit(server_round, results, failures)\n",
        "\n",
        "strategy = SaveModelStrategy(\n",
        "    fraction_fit=FRACTION_CLIENTS, fraction_evaluate=FRACTION_CLIENTS,\n",
        "    min_fit_clients=MINIMUM_CLIENTS, min_evaluate_clients=NUM_CLIENTS,\n",
        "    min_available_clients=MINIMUM_CLIENTS,\n",
        "    on_fit_config_fn=fit_config, on_evaluate_config_fn=eval_config,\n",
        "    evaluate_metrics_aggregation_fn=weighted_average,\n",
        "    initial_parameters=fl.common.ndarrays_to_parameters(model.get_weights()),\n",
        ")\n",
        "\n",
        "fl.simulation.start_simulation(\n",
        "    client_fn=client_fn, num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=NUM_ROUNDS), strategy=strategy\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT89IW40X8N-"
      },
      "source": [
        "Now that we have all dependencies installed, we can import everything we need for this tutorial:"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}